# Project Data Modeling with Postgres

This project is the first project of the Udacity Data Engineer Nanodegree Program. It contains data modeling with Postgres by building an ETL-Pipeline using Python. To complete the project, the students will need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL. 

## Purpose

The startup Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

With the help of the current data design, the basis for further evaluations has been laid and the analytics team can achieve their analytical goals.

## The schema description

The star schema includes the following tables:

**fact table** 
- sonplays

**dimension tables**
- users
- songs
- artists
- time

## How to run the scripts

To bring the data into the database you need to follow theses steps:
1) execute `sql_queries.py` => this file contains the SQL-Statements
2) execute `create_tables.py` => this file contains Python-Functions to create the database and the tables
3) execute  `etl.py` => this file contains the data integration

## Files in the repo

The repo contains two datasets:
- the song dataset: The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID
- the log dataset: Log Dataset The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## Example queries (optional)

Executing the following query `SELECT * FROM songplays where song_id is NOT NULL and artist_id is NOT NULL LIMIT 5;` should result just one dataset => and it does :)

## State and justify your database schema design and ETL pipeline.

The database schema design and the ETL pipeline were given by the task at hand. Using a star schema for reporting and analysis has proven itself in praxis. A lot of queries can be run on this schema properly to answer different questions.

In practice, however, problems can arise with some column names. Column names like *name* in the artists table or *level* in the users' table are reserved words in the sql syntax und should be replaced by using *artist_name* oder *user_level* for example. Adding foreign keys could improve data integrity across tables.